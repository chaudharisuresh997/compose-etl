version: '3.7'
services:
  tube:
    image: "quay.io/cdis/tube:feat_parser"
    command: bash -c "python run_config.py && sleep 50 && python run_import.py"
    networks:
      - devnet
    environment:
      - DICTIONARY_URL=https://s3.amazonaws.com/dictionary-artifacts/ndhdictionary/master/schema.json
      - HADOOP_URL=hdfs://spark:9000
      - ES_URL=host.docker.internal
      - HADOOP_HOST=spark
    volumes:
      - ./apis_configs/tube_settings.py:/tube/tube/local_settings.py
      - ./apis_configs/creds.json:/tube/creds.json
      - ./apis_configs/config_helper.py:/tube/tube/config_helper.py
    depends_on:
      - spark
  spark:
    image: "quay.io/cdis/gen3-spark:feat_init"
    #build: ../gen3-spark
    command: bash -c "python run_config.py && hdfs namenode -format && hdfs --daemon start namenode && hdfs --daemon start datanode && yarn --daemon start resourcemanager && yarn --daemon start nodemanager && hdfs dfsadmin -safemode leave &&  hdfs dfs -mkdir /result && while true; do sleep 5; done"
    expose:
      - 22
      - 9000
      - 8030
      - 8031
      - 8032
    networks:
      - devnet
    environment:
      - HADOOP_URL=hdfs://0.0.0.0:9000
      - HADOOP_HOST=0.0.0.0
networks:
  devnet:
